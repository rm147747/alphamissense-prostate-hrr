{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bec4b2",
   "metadata": {},
   "source": [
    "# üß¨ AlphaMissense VUS Reclassification ‚Äî Prostate Cancer HRR Genes\n",
    "## Notebook 1: Data Download, Filtering & Annotation\n",
    "### v3 ‚Äî Optimized for GitHub Codespaces (uses cBioPortal API)\n",
    "\n",
    "**Runtime: ~5 min** | No GPU needed | All public data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471c7a3",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0724329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/alphamissense-prostate-hrr/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "for pkg in [\"pandas\", \"requests\", \"tqdm\", \"openpyxl\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os, io, re, gzip, json, warnings\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "for d in [DATA_DIR, RESULTS_DIR, DATA_DIR / \"raw\", DATA_DIR / \"processed\"]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fa5f1",
   "metadata": {},
   "source": [
    "## 2. HRR Gene Panel (PROfound / TRITON3 / TALAPRO-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca88ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort A: ['BRCA1', 'BRCA2', 'ATM']\n",
      "Cohort B: 11 genes\n",
      "Extended panel: 25 genes total\n"
     ]
    }
   ],
   "source": [
    "COHORT_A = [\"BRCA1\", \"BRCA2\", \"ATM\"]\n",
    "COHORT_B = [\"PALB2\",\"BRIP1\",\"BARD1\",\"CDK12\",\"CHEK1\",\"CHEK2\",\"FANCL\",\"RAD51B\",\"RAD51C\",\"RAD51D\",\"RAD54L\"]\n",
    "EXPANDED = [\"FANCA\",\"FANCC\",\"FANCD2\",\"FANCE\",\"FANCF\",\"FANCG\",\"NBN\",\"MRE11\",\"RAD50\",\"ATR\",\"ATRX\"]\n",
    "\n",
    "HRR_PRIMARY = sorted(set(COHORT_A + COHORT_B))\n",
    "HRR_ALL = sorted(set(HRR_PRIMARY + EXPANDED))\n",
    "\n",
    "GENE2UNI = {\n",
    "    \"BRCA1\":\"P38398\",\"BRCA2\":\"P51587\",\"ATM\":\"Q13315\",\"PALB2\":\"Q86YC2\",\n",
    "    \"BRIP1\":\"Q9BX63\",\"BARD1\":\"Q99728\",\"CDK12\":\"Q9NYV4\",\"CHEK1\":\"O14757\",\n",
    "    \"CHEK2\":\"O96017\",\"FANCL\":\"Q9NW38\",\"RAD51B\":\"O15315\",\"RAD51C\":\"O43502\",\n",
    "    \"RAD51D\":\"O75771\",\"RAD54L\":\"Q92698\",\"FANCA\":\"O15360\",\"FANCC\":\"Q00597\",\n",
    "    \"FANCD2\":\"Q9BXW9\",\"FANCE\":\"Q9HB96\",\"FANCF\":\"Q9NPI8\",\"FANCG\":\"O15287\",\n",
    "    \"NBN\":\"O60934\",\"MRE11\":\"P49959\",\"RAD50\":\"Q92878\",\"ATR\":\"Q13535\",\"ATRX\":\"P46100\",\n",
    "}\n",
    "UNI2GENE = {v: k for k, v in GENE2UNI.items()}\n",
    "\n",
    "print(f\"Cohort A: {COHORT_A}\")\n",
    "print(f\"Cohort B: {len(COHORT_B)} genes\")\n",
    "print(f\"Extended panel: {len(HRR_ALL)} genes total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b613ac",
   "metadata": {},
   "source": [
    "## 3. Download TCGA-PRAD Mutations\n",
    "Uses the **cBioPortal REST API** (not the S3 datahub, which is blocked in Codespaces).\n",
    "\n",
    "If you already ran the download script, this cell detects the cached file and skips the download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9051ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found cached mutation data: data/raw/tcga_prad_mutations_raw.csv\n",
      "   21,448 mutations, 491 samples\n"
     ]
    }
   ],
   "source": [
    "STUDY = \"prad_tcga_pan_can_atlas_2018\"\n",
    "CBIO = \"https://www.cbioportal.org/api\"\n",
    "cache_file = DATA_DIR / \"raw\" / \"tcga_prad_mutations_raw.csv\"\n",
    "\n",
    "if cache_file.exists() and os.path.getsize(cache_file) > 1000:\n",
    "    print(f\"üìÇ Found cached mutation data: {cache_file}\")\n",
    "    df_mut_raw = pd.read_csv(cache_file, low_memory=False)\n",
    "    print(f\"   {len(df_mut_raw):,} mutations, {df_mut_raw['sampleId'].nunique()} samples\")\n",
    "else:\n",
    "    print(\"üì• Downloading mutations via cBioPortal API...\")\n",
    "    profile_id = f\"{STUDY}_mutations\"\n",
    "    \n",
    "    r = requests.post(\n",
    "        f\"{CBIO}/molecular-profiles/{profile_id}/mutations/fetch\",\n",
    "        headers={\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"},\n",
    "        json={\"sampleListId\": f\"{STUDY}_all\"},\n",
    "        params={\"projection\": \"DETAILED\"},\n",
    "        timeout=300\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    \n",
    "    df_mut_raw = pd.json_normalize(data)\n",
    "    df_mut_raw.to_csv(cache_file, index=False)\n",
    "    print(f\"‚úÖ Downloaded {len(df_mut_raw):,} mutations, {df_mut_raw['sampleId'].nunique()} samples\")\n",
    "    print(f\"   üíæ Cached to {cache_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b274e",
   "metadata": {},
   "source": [
    "## 4. Filter: Missense Mutations in HRR Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678c551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column format: gene=gene.hugoGeneSymbol, class=mutationType\n",
      "Total mutations: 21,448\n",
      "\n",
      "In HRR genes: 78\n",
      "Missense only: 52\n",
      "Unique patients: 40\n",
      "\n",
      "        Gene Cohort     N\n",
      "----------------------------\n",
      "         ATM      A    15\n",
      "       CDK12      B     5\n",
      "       BARD1      B     4\n",
      "      RAD51B      B     3\n",
      "       BRCA2      A     3\n",
      "        ATRX    Ext     3\n",
      "       PALB2      B     3\n",
      "       BRIP1      B     2\n",
      "      RAD54L      B     2\n",
      "         ATR    Ext     2\n",
      "         NBN    Ext     2\n",
      "       FANCG    Ext     1\n",
      "      RAD51D      B     1\n",
      "      FANCD2    Ext     1\n",
      "       FANCL      B     1\n",
      "       FANCC    Ext     1\n",
      "       RAD50    Ext     1\n",
      "       BRCA1      A     1\n",
      "       FANCF    Ext     1\n"
     ]
    }
   ],
   "source": [
    "# Detect column names (API vs MAF format)\n",
    "if \"gene.hugoGeneSymbol\" in df_mut_raw.columns:\n",
    "    G = \"gene.hugoGeneSymbol\"\n",
    "    CLS = \"mutationType\"\n",
    "    SAM = \"sampleId\"\n",
    "    HGV = \"proteinChange\"\n",
    "elif \"Hugo_Symbol\" in df_mut_raw.columns:\n",
    "    G = \"Hugo_Symbol\"\n",
    "    CLS = \"Variant_Classification\"\n",
    "    SAM = \"Tumor_Sample_Barcode\"\n",
    "    HGV = \"HGVSp_Short\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown columns: {list(df_mut_raw.columns[:10])}\")\n",
    "\n",
    "print(f\"Column format: gene={G}, class={CLS}\")\n",
    "print(f\"Total mutations: {len(df_mut_raw):,}\\n\")\n",
    "\n",
    "# Filter HRR genes\n",
    "df_hrr = df_mut_raw[df_mut_raw[G].isin(HRR_ALL)].copy()\n",
    "print(f\"In HRR genes: {len(df_hrr):,}\")\n",
    "\n",
    "# Filter missense\n",
    "df_miss = df_hrr[df_hrr[CLS].str.contains(\"issense\", case=False, na=False)].copy()\n",
    "print(f\"Missense only: {len(df_miss):,}\")\n",
    "print(f\"Unique patients: {df_miss[SAM].nunique()}\\n\")\n",
    "\n",
    "# Per-gene summary\n",
    "print(f\"{'Gene':>12s} {'Cohort':>6s} {'N':>5s}\")\n",
    "print(\"-\" * 28)\n",
    "for gene, n in df_miss[G].value_counts().items():\n",
    "    c = \"A\" if gene in COHORT_A else (\"B\" if gene in COHORT_B else \"Ext\")\n",
    "    print(f\"{gene:>12s} {c:>6s} {n:>5d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c052158",
   "metadata": {},
   "source": [
    "## 5. Parse Protein Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bb9e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed: 52 variants\n",
      "   UniProt mapped: 52\n",
      "\n",
      "Examples:\n",
      "        ATM           G2695S  ‚Üí  Q13315_2695_G_S\n",
      "        ATM           G1672A  ‚Üí  Q13315_1672_G_A\n",
      "      BRCA2           N1435T  ‚Üí  P51587_1435_N_T\n",
      "      FANCG            R353S  ‚Üí  O15287_353_R_S\n",
      "        ATM           E2164K  ‚Üí  Q13315_2164_E_K\n"
     ]
    }
   ],
   "source": [
    "def parse_hgvsp(s):\n",
    "    if pd.isna(s): return None, None, None\n",
    "    s = str(s).strip()\n",
    "    aa3 = {'Ala':'A','Arg':'R','Asn':'N','Asp':'D','Cys':'C','Gln':'Q','Glu':'E',\n",
    "           'Gly':'G','His':'H','Ile':'I','Leu':'L','Lys':'K','Met':'M','Phe':'F',\n",
    "           'Pro':'P','Ser':'S','Thr':'T','Trp':'W','Tyr':'Y','Val':'V','Ter':'*'}\n",
    "    # 3-letter: p.Arg175His\n",
    "    m = re.match(r'p\\.([A-Z][a-z]{2})(\\d+)([A-Z][a-z]{2})', s)\n",
    "    if m:\n",
    "        r_, a_ = aa3.get(m.group(1)), aa3.get(m.group(3))\n",
    "        if r_ and a_ and r_ != a_: return r_, int(m.group(2)), a_\n",
    "    # 1-letter: p.R175H or R175H\n",
    "    m = re.match(r'(?:p\\.)?([A-Z*])(\\d+)([A-Z*])', s)\n",
    "    if m and m.group(1) != m.group(3):\n",
    "        return m.group(1), int(m.group(2)), m.group(3)\n",
    "    return None, None, None\n",
    "\n",
    "parsed = df_miss[HGV].apply(parse_hgvsp)\n",
    "df_miss = df_miss.copy()\n",
    "df_miss[\"ref_aa\"] = [p[0] for p in parsed]\n",
    "df_miss[\"protein_pos\"] = [p[1] for p in parsed]\n",
    "df_miss[\"alt_aa\"] = [p[2] for p in parsed]\n",
    "\n",
    "# Keep only parsed\n",
    "df_miss = df_miss.dropna(subset=[\"ref_aa\",\"protein_pos\",\"alt_aa\"]).copy()\n",
    "df_miss[\"protein_pos\"] = df_miss[\"protein_pos\"].astype(int)\n",
    "\n",
    "# UniProt mapping + key\n",
    "df_miss[\"uniprot_id\"] = df_miss[G].map(GENE2UNI)\n",
    "df_miss[\"am_key\"] = (\n",
    "    df_miss[\"uniprot_id\"] + \"_\" +\n",
    "    df_miss[\"protein_pos\"].astype(str) + \"_\" +\n",
    "    df_miss[\"ref_aa\"] + \"_\" +\n",
    "    df_miss[\"alt_aa\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Parsed: {len(df_miss)} variants\")\n",
    "print(f\"   UniProt mapped: {df_miss['uniprot_id'].notna().sum()}\")\n",
    "print(f\"\\nExamples:\")\n",
    "for _, r in df_miss.head(5).iterrows():\n",
    "    print(f\"   {r[G]:>8s}  {str(r[HGV]):>15s}  ‚Üí  {r['am_key']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2733d5",
   "metadata": {},
   "source": [
    "## 6. Download AlphaMissense Predictions (Lightweight ‚ö°)\n",
    "\n",
    "Instead of the full 450MB file, we try:\n",
    "1. **Per-protein files** from Google Storage (few KB each)\n",
    "2. **Fallback:** streaming the hg38 file and filtering on-the-fly\n",
    "3. **Manual fallback:** upload from Zenodo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0293feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found cached AlphaMissense data: data/processed/alphamissense_hrr_genes.csv\n",
      "   554,363 predictions loaded\n",
      "\n",
      "        Gene  Total   Path Benign  Ambig  %Path\n",
      "==================================================\n",
      "         ATM  58083  22037  26627   9419  37.9%\n",
      "         ATR  50236  23767  19418   7051  47.3%\n",
      "        ATRX  47367  24503  18114   4750  51.7%\n",
      "       BARD1  14763   4385   8531   1847  29.7%\n",
      "       BRCA1  35397   5616  24611   5170  15.9%\n",
      "       BRCA2  64961  10158  45731   9072  15.6%\n",
      "       BRIP1  23731   7485  13841   2405  31.5%\n",
      "       CDK12  28310  11594  12888   3828  41.0%\n",
      "       CHEK1   9044   5252   2764   1028  58.1%\n",
      "       CHEK2  10317   5245   3895   1177  50.8%\n",
      "       FANCA  27645   8142  14736   4767  29.5%\n",
      "       FANCC  10602   2698   5596   2308  25.4%\n",
      "      FANCD2  27569   8977  14428   4164  32.6%\n",
      "       FANCE  10184   2348   6096   1740  23.1%\n",
      "       FANCF   7106   2089   3732   1285  29.4%\n",
      "       FANCG  11818   3029   6688   2101  25.6%\n",
      "       FANCL   7125   2826   3001   1298  39.7%\n",
      "       MRE11  13452   6052   5651   1749  45.0%\n",
      "         NBN  14326   3682   8881   1763  25.7%\n",
      "       PALB2  22534   4303  15019   3212  19.1%\n",
      "       RAD50  24928  11884   9055   3989  47.7%\n",
      "      RAD51B   7296   2934   3363    999  40.2%\n",
      "      RAD51C   7144   2933   3201   1010  41.1%\n",
      "      RAD51D   6232   2730   2504    998  43.8%\n",
      "      RAD54L  14193   7857   4752   1584  55.4%\n",
      "\n",
      "   Total: 554,363 variant predictions\n"
     ]
    }
   ],
   "source": [
    "am_cache = DATA_DIR / \"processed\" / \"alphamissense_hrr_genes.csv\"\n",
    "\n",
    "if am_cache.exists() and os.path.getsize(am_cache) > 1000:\n",
    "    print(f\"üìÇ Found cached AlphaMissense data: {am_cache}\")\n",
    "    df_am = pd.read_csv(am_cache)\n",
    "    print(f\"   {len(df_am):,} predictions loaded\")\n",
    "else:\n",
    "    print(\"üì• Downloading AlphaMissense per-protein predictions...\")\n",
    "    print(f\"   {len(GENE2UNI)} proteins to fetch\\n\")\n",
    "    \n",
    "    AM_BASE = \"https://storage.googleapis.com/dm_alphamissense\"\n",
    "    all_am = []\n",
    "    failed = []\n",
    "    \n",
    "    for gene, uid in tqdm(GENE2UNI.items(), desc=\"Fetching\"):\n",
    "        try:\n",
    "            # Try the aa_substitutions per-protein endpoint\n",
    "            url = f\"{AM_BASE}/AlphaMissense_aa_substitutions.tsv.gz\"\n",
    "            # This is the big file ‚Äî skip, try hg38 per-gene approach\n",
    "            raise Exception(\"Skip big file\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Alternative: query the AlphaFold DB for AM scores\n",
    "        # The per-protein TSVs aren't individually hosted, so we need the big file\n",
    "        failed.append(gene)\n",
    "    \n",
    "    if len(all_am) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  Per-protein download not available individually.\")\n",
    "        print(\"   Downloading full AlphaMissense file (streaming + filtering)...\")\n",
    "        print(\"   This downloads ~450MB but only keeps HRR genes in memory.\\n\")\n",
    "        \n",
    "        target_uniprots = set(GENE2UNI.values())\n",
    "        \n",
    "        try:\n",
    "            url = f\"{AM_BASE}/AlphaMissense_aa_substitutions.tsv.gz\"\n",
    "            resp = requests.get(url, stream=True, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            \n",
    "            total = int(resp.headers.get('content-length', 0))\n",
    "            gz_path = DATA_DIR / \"raw\" / \"AlphaMissense_aa_substitutions.tsv.gz\"\n",
    "            downloaded = 0\n",
    "            \n",
    "            with open(gz_path, 'wb') as f:\n",
    "                for chunk in resp.iter_content(chunk_size=1024*1024):\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total > 0:\n",
    "                        print(f\"   {downloaded/1e6:.0f}/{total/1e6:.0f} MB ({100*downloaded/total:.0f}%)\", end=\"\\r\")\n",
    "            \n",
    "            print(f\"\\n   ‚úÖ Downloaded: {gz_path}\")\n",
    "            print(\"   Parsing (filtering for HRR genes only)...\\n\")\n",
    "            \n",
    "            with gzip.open(gz_path, 'rt') as f:\n",
    "                header = None\n",
    "                n_lines = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('#'): continue\n",
    "                    if header is None:\n",
    "                        header = line.strip().split('\\t')\n",
    "                        continue\n",
    "                    n_lines += 1\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 4 and parts[0] in target_uniprots:\n",
    "                        try:\n",
    "                            all_am.append({\n",
    "                                \"uniprot_id\": parts[0],\n",
    "                                \"gene\": UNI2GENE.get(parts[0], parts[0]),\n",
    "                                \"protein_variant\": parts[1],\n",
    "                                \"am_pathogenicity\": float(parts[2]),\n",
    "                                \"am_class\": parts[3].strip(),\n",
    "                            })\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                    if n_lines % 5_000_000 == 0:\n",
    "                        print(f\"   {n_lines/1e6:.0f}M lines | {len(all_am)} HRR variants\", end=\"\\r\")\n",
    "            \n",
    "            print(f\"\\n   ‚úÖ Extracted {len(all_am):,} from {n_lines:,} lines\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Download failed: {e}\")\n",
    "            print(\"\\nüìù MANUAL OPTION:\")\n",
    "            print(\"   1. Download from: https://zenodo.org/records/8208688\")\n",
    "            print(\"   2. Get: AlphaMissense_aa_substitutions.tsv.gz\")\n",
    "            print(\"   3. Upload to: data/raw/ in your Codespace\")\n",
    "            print(\"   4. Re-run this cell\")\n",
    "            \n",
    "            manual = DATA_DIR / \"raw\" / \"AlphaMissense_aa_substitutions.tsv.gz\"\n",
    "            if manual.exists():\n",
    "                print(\"\\n‚úÖ Found manually uploaded file! Parsing...\")\n",
    "                target_uniprots = set(GENE2UNI.values())\n",
    "                with gzip.open(manual, 'rt') as f:\n",
    "                    header = None\n",
    "                    for line in f:\n",
    "                        if line.startswith('#'): continue\n",
    "                        if header is None: header = line; continue\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 4 and parts[0] in target_uniprots:\n",
    "                            try:\n",
    "                                all_am.append({\n",
    "                                    \"uniprot_id\": parts[0],\n",
    "                                    \"gene\": UNI2GENE.get(parts[0], parts[0]),\n",
    "                                    \"protein_variant\": parts[1],\n",
    "                                    \"am_pathogenicity\": float(parts[2]),\n",
    "                                    \"am_class\": parts[3].strip(),\n",
    "                                })\n",
    "                            except ValueError: continue\n",
    "                print(f\"‚úÖ Extracted {len(all_am):,} HRR variants\")\n",
    "    \n",
    "    # Process into DataFrame\n",
    "    if len(all_am) > 0:\n",
    "        df_am = pd.DataFrame(all_am)\n",
    "        df_am[\"ref_aa_am\"] = df_am[\"protein_variant\"].str[0]\n",
    "        df_am[\"alt_aa_am\"] = df_am[\"protein_variant\"].str[-1]\n",
    "        df_am[\"pos_str\"] = df_am[\"protein_variant\"].str[1:-1]\n",
    "        df_am = df_am[df_am[\"pos_str\"].str.match(r'^\\d+$', na=False)].copy()\n",
    "        df_am[\"protein_pos_am\"] = df_am[\"pos_str\"].astype(int)\n",
    "        df_am[\"am_key\"] = (\n",
    "            df_am[\"uniprot_id\"] + \"_\" +\n",
    "            df_am[\"protein_pos_am\"].astype(str) + \"_\" +\n",
    "            df_am[\"ref_aa_am\"] + \"_\" +\n",
    "            df_am[\"alt_aa_am\"]\n",
    "        )\n",
    "        df_am = df_am.drop_duplicates(\"am_key\")\n",
    "        df_am.to_csv(am_cache, index=False)\n",
    "        print(f\"\\nüíæ Cached: {am_cache}\")\n",
    "    else:\n",
    "        df_am = pd.DataFrame(columns=[\"am_key\",\"am_pathogenicity\",\"am_class\"])\n",
    "        print(\"\\n‚ö†Ô∏è  No AlphaMissense data available yet\")\n",
    "\n",
    "# Summary\n",
    "if len(df_am) > 0:\n",
    "    print(f\"\\n{'Gene':>12s} {'Total':>6s} {'Path':>6s} {'Benign':>6s} {'Ambig':>6s} {'%Path':>6s}\")\n",
    "    print(\"=\" * 50)\n",
    "    for gene in sorted(GENE2UNI.keys()):\n",
    "        s = df_am[df_am[\"gene\"]==gene]\n",
    "        if len(s)==0: continue\n",
    "        np_=((s[\"am_class\"]==\"pathogenic\").sum())\n",
    "        nb_=((s[\"am_class\"]==\"benign\").sum())\n",
    "        na_=((s[\"am_class\"]==\"ambiguous\").sum())\n",
    "        print(f\"{gene:>12s} {len(s):>6d} {np_:>6d} {nb_:>6d} {na_:>6d} {100*np_/len(s):>5.1f}%\")\n",
    "    print(f\"\\n   Total: {len(df_am):,} variant predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285d498",
   "metadata": {},
   "source": [
    "## 7. Download ClinVar Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf2563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found cached ClinVar data\n",
      "\n",
      "Distribution:\n",
      "               VUS: 80630 (41.7%)\n",
      "              LB/B: 60264 (31.2%)\n",
      "       Conflicting: 27046 (14.0%)\n",
      "        Pathogenic:  7449 (3.9%)\n",
      "            Benign:  6455 (3.3%)\n",
      "              LP/P:  6008 (3.1%)\n",
      "             Other:  5296 (2.7%)\n"
     ]
    }
   ],
   "source": [
    "clinvar_cache = DATA_DIR / \"processed\" / \"clinvar_hrr.csv\"\n",
    "\n",
    "if clinvar_cache.exists() and os.path.getsize(clinvar_cache) > 500:\n",
    "    print(f\"üìÇ Found cached ClinVar data\")\n",
    "    df_clinvar = pd.read_csv(clinvar_cache)\n",
    "else:\n",
    "    print(\"üì• Downloading ClinVar...\")\n",
    "    CLINVAR_URL = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz\"\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(CLINVAR_URL, timeout=300, stream=True)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        cv_file = DATA_DIR / \"raw\" / \"variant_summary.txt.gz\"\n",
    "        with open(cv_file, 'wb') as f:\n",
    "            for chunk in resp.iter_content(chunk_size=1024*1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"   Downloaded: {os.path.getsize(cv_file)/1e6:.1f} MB\")\n",
    "        \n",
    "        records = []\n",
    "        with gzip.open(cv_file, 'rt', errors='replace') as f:\n",
    "            header = f.readline().strip().split('\\t')\n",
    "            idx = {c: i for i, c in enumerate(header)}\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                gene = parts[idx.get('GeneSymbol',0)] if 'GeneSymbol' in idx else \"\"\n",
    "                if gene not in HRR_ALL: continue\n",
    "                vtype = parts[idx.get('Type',0)] if 'Type' in idx else \"\"\n",
    "                name = parts[idx.get('Name',0)] if 'Name' in idx else \"\"\n",
    "                if \"single nucleotide\" in vtype.lower() or \"missense\" in name.lower():\n",
    "                    records.append({\n",
    "                        'cv_gene': gene,\n",
    "                        'cv_name': name,\n",
    "                        'cv_significance': parts[idx.get('ClinicalSignificance',0)] if 'ClinicalSignificance' in idx else \"\",\n",
    "                    })\n",
    "        \n",
    "        df_clinvar = pd.DataFrame(records)\n",
    "        \n",
    "        def simplify(sig):\n",
    "            s = str(sig).lower()\n",
    "            if \"pathogenic\" in s and \"conflicting\" not in s:\n",
    "                return \"LP/P\" if \"likely\" in s else \"Pathogenic\"\n",
    "            elif \"benign\" in s and \"conflicting\" not in s:\n",
    "                return \"LB/B\" if \"likely\" in s else \"Benign\"\n",
    "            elif \"uncertain\" in s: return \"VUS\"\n",
    "            elif \"conflicting\" in s: return \"Conflicting\"\n",
    "            return \"Other\"\n",
    "        \n",
    "        df_clinvar[\"cv_class\"] = df_clinvar[\"cv_significance\"].apply(simplify)\n",
    "        df_clinvar.to_csv(clinvar_cache, index=False)\n",
    "        print(f\"\\n‚úÖ ClinVar HRR variants: {len(df_clinvar):,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  ClinVar failed: {e}\")\n",
    "        df_clinvar = pd.DataFrame()\n",
    "\n",
    "if len(df_clinvar) > 0:\n",
    "    print(f\"\\nDistribution:\")\n",
    "    for cls, n in df_clinvar[\"cv_class\"].value_counts().items():\n",
    "        print(f\"   {cls:>15s}: {n:5d} ({100*n/len(df_clinvar):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667c636",
   "metadata": {},
   "source": [
    "## 8. Merge: Mutations √ó AlphaMissense √ó ClinVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b374613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Merging datasets...\n",
      "\n",
      "‚úÖ AlphaMissense match: 51/52 (98.1%)\n",
      "\n",
      "AlphaMissense classification:\n",
      "            benign:  31 ( 59.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "        pathogenic:  19 ( 36.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "         ambiguous:   1 (  1.9%) \n"
     ]
    }
   ],
   "source": [
    "print(\"üîó Merging datasets...\\n\")\n",
    "\n",
    "if len(df_am) > 0:\n",
    "    df_ann = df_miss.merge(\n",
    "        df_am[[\"am_key\",\"am_pathogenicity\",\"am_class\"]].drop_duplicates(\"am_key\"),\n",
    "        on=\"am_key\", how=\"left\"\n",
    "    )\n",
    "    matched = df_ann[\"am_pathogenicity\"].notna().sum()\n",
    "    print(f\"‚úÖ AlphaMissense match: {matched}/{len(df_ann)} ({100*matched/len(df_ann):.1f}%)\")\n",
    "else:\n",
    "    df_ann = df_miss.copy()\n",
    "    df_ann[\"am_pathogenicity\"] = np.nan\n",
    "    df_ann[\"am_class\"] = \"not_annotated\"\n",
    "    print(\"‚ö†Ô∏è  AlphaMissense not loaded ‚Äî placeholder columns added\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nAlphaMissense classification:\")\n",
    "for cls, n in df_ann[\"am_class\"].value_counts().items():\n",
    "    bar = \"‚ñà\" * int(50 * n / len(df_ann))\n",
    "    print(f\"   {cls:>15s}: {n:3d} ({100*n/len(df_ann):5.1f}%) {bar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e454e6",
   "metadata": {},
   "source": [
    "## 9. Download Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a645d431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Clinical data already downloaded\n",
      "\n",
      "Patients: 15949\n",
      "Columns: ['uniquePatientKey', 'patientId', 'studyId', 'clinicalAttributeId', 'value']\n"
     ]
    }
   ],
   "source": [
    "clin_cache = DATA_DIR / \"raw\" / \"clinical_patient.csv\"\n",
    "\n",
    "if clin_cache.exists():\n",
    "    print(\"üìÇ Clinical data already downloaded\")\n",
    "    df_clin_pat = pd.read_csv(clin_cache)\n",
    "    df_clin_sam = pd.read_csv(DATA_DIR / \"raw\" / \"clinical_sample.csv\")\n",
    "else:\n",
    "    print(\"üì• Downloading clinical data via API...\")\n",
    "    for ctype, fname, varname in [\n",
    "        (\"PATIENT\", \"clinical_patient.csv\", \"df_clin_pat\"),\n",
    "        (\"SAMPLE\", \"clinical_sample.csv\", \"df_clin_sam\"),\n",
    "    ]:\n",
    "        r = requests.get(\n",
    "            f\"{CBIO}/studies/{STUDY}/clinical-data?clinicalDataType={ctype}\",\n",
    "            headers={\"Accept\": \"application/json\"}, timeout=60\n",
    "        )\n",
    "        df_tmp = pd.json_normalize(r.json())\n",
    "        df_tmp.to_csv(DATA_DIR / \"raw\" / fname, index=False)\n",
    "        print(f\"   ‚úÖ {fname}: {len(df_tmp)} rows\")\n",
    "        if ctype == \"PATIENT\": df_clin_pat = df_tmp\n",
    "        else: df_clin_sam = df_tmp\n",
    "\n",
    "print(f\"\\nPatients: {len(df_clin_pat)}\")\n",
    "surv = [c for c in df_clin_pat.columns if any(x in c.upper() for x in [\"OS\",\"DFS\",\"PFS\",\"SURV\",\"STATUS\"])]\n",
    "if surv:\n",
    "    print(f\"Survival columns: {surv}\")\n",
    "else:\n",
    "    print(f\"Columns: {list(df_clin_pat.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb40e39",
   "metadata": {},
   "source": [
    "## 10. Final Annotated Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb91f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SAVED: results/annotated_hrr_variants.csv\n",
      "   Rows: 52\n",
      "   Unique variants: 52\n",
      "   Unique patients: 40\n",
      "   Genes: 19\n",
      "\n",
      "By cohort:\n",
      "   A: 19 variants, 3 genes\n",
      "   B: 21 variants, 8 genes\n",
      "   Ext: 12 variants, 8 genes\n",
      "\n",
      "Preview:\n",
      "      sample_id gene protein_change  am_pathogenicity   am_class hrr_cohort\n",
      "TCGA-KK-A8IF-01  ATM          R337C            0.3232     benign          A\n",
      "TCGA-J4-A67Q-01  ATM         L1078V            0.2514     benign          A\n",
      "TCGA-EJ-7784-01  ATM         H1568L            0.0822     benign          A\n",
      "TCGA-CH-5762-01  ATM         G1672A            0.1741     benign          A\n",
      "TCGA-M7-A725-01  ATM         I1846N            0.6880 pathogenic          A\n",
      "TCGA-EJ-5518-01  ATM         L1936S            0.8693 pathogenic          A\n",
      "TCGA-EJ-5511-01  ATM         E2164K            0.8689 pathogenic          A\n",
      "TCGA-VN-A88K-01  ATM         R2453P            0.7065 pathogenic          A\n",
      "TCGA-HC-8260-01  ATM         G2694R            0.9621 pathogenic          A\n",
      "TCGA-CH-5737-01  ATM         G2695S            0.9738 pathogenic          A\n"
     ]
    }
   ],
   "source": [
    "# Build final table with clean column names\n",
    "df_final = df_ann[[\n",
    "    c for c in [SAM, G, HGV, CLS, \"chr\", \"startPosition\", \"referenceAllele\", \"variantAllele\",\n",
    "                \"ref_aa\", \"protein_pos\", \"alt_aa\", \"uniprot_id\", \"am_key\",\n",
    "                \"am_pathogenicity\", \"am_class\",\n",
    "                \"Chromosome\", \"Start_Position\", \"Reference_Allele\", \"Tumor_Seq_Allele2\"]\n",
    "    if c in df_ann.columns\n",
    "]].copy()\n",
    "\n",
    "# Standardize names\n",
    "rename = {SAM: \"sample_id\", G: \"gene\", HGV: \"protein_change\", CLS: \"variant_classification\"}\n",
    "df_final = df_final.rename(columns={k:v for k,v in rename.items() if k in df_final.columns})\n",
    "\n",
    "# Add cohort\n",
    "df_final[\"hrr_cohort\"] = df_final[\"gene\"].apply(\n",
    "    lambda g: \"A\" if g in COHORT_A else (\"B\" if g in COHORT_B else \"Ext\"))\n",
    "\n",
    "df_final = df_final.sort_values([\"gene\",\"protein_pos\",\"sample_id\"]).reset_index(drop=True)\n",
    "df_final.to_csv(RESULTS_DIR / \"annotated_hrr_variants.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ SAVED: results/annotated_hrr_variants.csv\")\n",
    "print(f\"   Rows: {len(df_final):,}\")\n",
    "print(f\"   Unique variants: {df_final['am_key'].nunique():,}\")\n",
    "print(f\"   Unique patients: {df_final['sample_id'].nunique():,}\")\n",
    "print(f\"   Genes: {df_final['gene'].nunique()}\")\n",
    "\n",
    "print(f\"\\nBy cohort:\")\n",
    "for c in [\"A\",\"B\",\"Ext\"]:\n",
    "    s = df_final[df_final[\"hrr_cohort\"]==c]\n",
    "    print(f\"   {c}: {len(s)} variants, {s['gene'].nunique()} genes\")\n",
    "\n",
    "print(f\"\\nPreview:\")\n",
    "show = [\"sample_id\",\"gene\",\"protein_change\",\"am_pathogenicity\",\"am_class\",\"hrr_cohort\"]\n",
    "show = [c for c in show if c in df_final.columns]\n",
    "print(df_final[show].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e480e5",
   "metadata": {},
   "source": [
    "## 11. Patient-Level Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94606a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients with HRR missense: 40\n",
      "  ‚â•1 AM-pathogenic: 19\n",
      "  All AM-benign:    21\n",
      "\n",
      "üíæ Saved: results/patient_hrr_summary.csv\n"
     ]
    }
   ],
   "source": [
    "if df_final[\"am_pathogenicity\"].notna().any():\n",
    "    pat = df_final.groupby(\"sample_id\").agg(\n",
    "        n_hrr_missense=(\"gene\",\"count\"),\n",
    "        n_pathogenic=(\"am_class\", lambda x: (x==\"pathogenic\").sum()),\n",
    "        n_benign=(\"am_class\", lambda x: (x==\"benign\").sum()),\n",
    "        n_ambiguous=(\"am_class\", lambda x: (x==\"ambiguous\").sum()),\n",
    "        max_am_score=(\"am_pathogenicity\",\"max\"),\n",
    "        hrr_genes=(\"gene\", lambda x: \", \".join(sorted(x.unique()))),\n",
    "        has_cohort_a=(\"hrr_cohort\", lambda x: (x==\"A\").any()),\n",
    "    ).reset_index()\n",
    "    pat[\"has_am_pathogenic\"] = pat[\"n_pathogenic\"] > 0\n",
    "    \n",
    "    pat.to_csv(RESULTS_DIR / \"patient_hrr_summary.csv\", index=False)\n",
    "    \n",
    "    print(f\"Patients with HRR missense: {len(pat)}\")\n",
    "    print(f\"  ‚â•1 AM-pathogenic: {pat['has_am_pathogenic'].sum()}\")\n",
    "    print(f\"  All AM-benign:    {(pat['n_pathogenic']==0).sum()}\")\n",
    "    print(f\"\\nüíæ Saved: results/patient_hrr_summary.csv\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Pending AlphaMissense annotation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17dcf8",
   "metadata": {},
   "source": [
    "## ‚úÖ Notebook 1 Complete!\n",
    "\n",
    "### Files:\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `results/annotated_hrr_variants.csv` | HRR missense variants + AlphaMissense scores |\n",
    "| `results/patient_hrr_summary.csv` | Patient-level summary |\n",
    "| `data/raw/clinical_patient.csv` | Survival & clinical data |\n",
    "| `data/processed/alphamissense_hrr_genes.csv` | Reusable AM lookup for HRR genes |\n",
    "\n",
    "### Git save:\n",
    "```bash\n",
    "git add -A && git commit -m \"Notebook 1: annotated HRR variants\" && git push\n",
    "```\n",
    "\n",
    "### Next ‚Üí Notebook 2: Statistical Analysis\n",
    "- Cox PH, Kaplan-Meier\n",
    "- AlphaMissense vs ClinVar concordance  \n",
    "- Sensitivity analyses\n",
    "- Publication figures\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
